{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Imports"
      ],
      "metadata": {
        "id": "cDyagaIpEgbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.special import expit as sigmoid\n",
        "from itertools import combinations\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# XGBoost random state is enabled in instantiation, ditto train_test_split\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(42)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "id": "4BosvvACEieJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "EsQ6KO4HHQXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load UCI Drug Review Dataset from: https://archive.ics.uci.edu/dataset/461/drug+review+dataset+druglib+com\n",
        "\n",
        "df_train = pd.read_csv('drugLibTrain_raw.tsv', sep='\\t')\n",
        "df_test = pd.read_csv('drugLibTest_raw.tsv', sep='\\t')\n",
        "df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df = df.rename(columns={'urlDrugName': 'drugName', 'Unnamed: 0': 'id'})\n",
        "\n",
        "print(f\"Total reviews: {len(df)}\")\n",
        "\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "W3kN12ERHUI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['drugName', 'condition', 'rating', 'id'])\n",
        "\n",
        "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
        "df = df[(df['rating'] >= 1) & (df['rating'] <= 10)]\n",
        "\n",
        "df['condition'] = df['condition'].str.lower().str.strip()\n",
        "\n",
        "top_conditions = df['condition'].value_counts().head(25).index\n",
        "df = df[df['condition'].isin(top_conditions)]\n",
        "\n",
        "drug_counts = df['drugName'].value_counts()\n",
        "valid_drugs = drug_counts[drug_counts >= 20].index\n",
        "df = df[df['drugName'].isin(valid_drugs)]\n",
        "\n",
        "print(f\"\\nAfter cleaning: {len(df)} reviews\")\n",
        "print(f\"Conditions: {df['condition'].nunique()}\")\n",
        "print(f\"Drugs: {df['drugName'].nunique()}\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cqhO6Y2mHaPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df['rating'], width = 0.7)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "Zv4sqfV8Cdnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Feature Engineering"
      ],
      "metadata": {
        "id": "PbnpQLKeHhsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SentenceTransformer model\n",
        "sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Sentiment Analysis Model (all-MiniLM) loaded\\n\")"
      ],
      "metadata": {
        "id": "A8f_uABaE8pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating semantic features...\")\n",
        "\n",
        "df['patient_id'] = range(len(df))\n",
        "df['reviews_combined'] = (df['benefitsReview'].fillna('') + ' ' + df['sideEffectsReview'].fillna('') + ' ' + df['commentsReview'].fillna(''))\n",
        "\n",
        "severity_anchors = {\n",
        "    'severe': [\n",
        "        \"severe symptoms\",\n",
        "        \"chronic condition\",\n",
        "        \"extreme pain\",\n",
        "        \"debilitating effects\",\n",
        "        \"intense suffering\",\n",
        "        \"acute distress\"\n",
        "    ],\n",
        "    'mild': [\n",
        "        \"mild symptoms\",\n",
        "        \"minor discomfort\",\n",
        "        \"slight improvement needed\",\n",
        "        \"moderate condition\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "side_effect_anchors = [\n",
        "    \"experienced side effects\",\n",
        "    \"adverse reactions occurred\",\n",
        "    \"medication caused nausea\",\n",
        "    \"felt dizzy and tired\",\n",
        "    \"weight gain from drug\",\n",
        "    \"insomnia and fatigue\",\n",
        "    \"headaches and discomfort\"\n",
        "]\n",
        "\n",
        "duration_anchors = {\n",
        "    'short': [\n",
        "        \"just started taking\",\n",
        "        \"used for a few days\",\n",
        "        \"taking for one week\",\n",
        "        \"recently began treatment\"\n",
        "    ],\n",
        "    'long': [\n",
        "        \"taking for months\",\n",
        "        \"used for years\",\n",
        "        \"long-term medication\",\n",
        "        \"been on this for a long time\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "sentiment_anchors = {\n",
        "    'positive': [\n",
        "        \"great medication that helped\",\n",
        "        \"excellent results and effectiveness\",\n",
        "        \"amazing improvement in symptoms\",\n",
        "        \"worked wonderfully for me\",\n",
        "        \"highly effective treatment\"\n",
        "    ],\n",
        "    'negative': [\n",
        "        \"terrible experience with drug\",\n",
        "        \"horrible side effects\",\n",
        "        \"awful medication that failed\",\n",
        "        \"useless and ineffective\",\n",
        "        \"made symptoms worse\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Encode all anchor texts\n",
        "severity_severe_emb = sentence_transformer_model.encode(severity_anchors['severe'])\n",
        "severity_mild_emb = sentence_transformer_model.encode(severity_anchors['mild'])\n",
        "side_effect_emb = sentence_transformer_model.encode(side_effect_anchors)\n",
        "duration_short_emb = sentence_transformer_model.encode(duration_anchors['short'])\n",
        "duration_long_emb = sentence_transformer_model.encode(duration_anchors['long'])\n",
        "sentiment_pos_emb = sentence_transformer_model.encode(sentiment_anchors['positive'])\n",
        "sentiment_neg_emb = sentence_transformer_model.encode(sentiment_anchors['negative'])\n",
        "\n",
        "batch_size = 500\n",
        "n_batches = (len(df) + batch_size - 1) // batch_size\n",
        "\n",
        "severity_scores = []\n",
        "side_effect_scores = []\n",
        "long_term_scores = []\n",
        "sentiment_scores = []\n",
        "\n",
        "for i in range(n_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min((i + 1) * batch_size, len(df))\n",
        "    batch_reviews = df['reviews_combined'].iloc[start_idx:end_idx].tolist()\n",
        "\n",
        "    batch_embeddings = sentence_transformer_model.encode(batch_reviews, show_progress_bar=False)\n",
        "\n",
        "    # Severity Score (higher # => more severe)\n",
        "    severe_sim = cosine_similarity(batch_embeddings, severity_severe_emb).max(axis=1)\n",
        "    mild_sim = cosine_similarity(batch_embeddings, severity_mild_emb).max(axis=1)\n",
        "    batch_severity = severe_sim / (severe_sim + mild_sim + 1e-8)\n",
        "    severity_scores.extend(batch_severity)\n",
        "\n",
        "    # Side Effect Score (higher # => more side effects)\n",
        "    side_effect_sim = cosine_similarity(batch_embeddings, side_effect_emb).max(axis=1)\n",
        "    side_effect_scores.extend(side_effect_sim)\n",
        "\n",
        "    # Duration Score (higher # => longer usage of drug treatment)\n",
        "    long_sim = cosine_similarity(batch_embeddings, duration_long_emb).max(axis=1)\n",
        "    short_sim = cosine_similarity(batch_embeddings, duration_short_emb).max(axis=1)\n",
        "    batch_duration = long_sim / (long_sim + short_sim + 1e-8)\n",
        "    long_term_scores.extend(batch_duration)\n",
        "\n",
        "    # Overall Sentiment Score (higher # => more positive)\n",
        "    pos_sim = cosine_similarity(batch_embeddings, sentiment_pos_emb).max(axis=1)\n",
        "    neg_sim = cosine_similarity(batch_embeddings, sentiment_neg_emb).max(axis=1)\n",
        "    batch_sentiment = pos_sim / (pos_sim + neg_sim + 1e-8)\n",
        "    sentiment_scores.extend(batch_sentiment)\n",
        "\n",
        "df['severity_score'] = severity_scores\n",
        "df['mentions_side_effects'] = side_effect_scores\n",
        "df['long_term_use'] = long_term_scores\n",
        "df['sentiment_score'] = sentiment_scores\n",
        "\n",
        "df['review_length'] = df['reviews_combined'].str.len()\n",
        "df['review_length_log'] = np.log1p(df['review_length'])\n",
        "\n",
        "print(f\"\\nFeature distributions:\")\n",
        "sentiment_cols = [\n",
        "    'severity_score',\n",
        "    'mentions_side_effects',\n",
        "    'long_term_use',\n",
        "    'sentiment_score',\n",
        "    'review_length'\n",
        "]\n",
        "\n",
        "print(df[sentiment_cols].agg(['mean', 'std']).T.head())\n",
        "\n",
        "high_severity_idx = df['severity_score'].nlargest(3).index\n",
        "high_side_effect_idx = df['mentions_side_effects'].nlargest(3).index\n",
        "\n",
        "print(\"\\n1. HIGH SEVERITY EXAMPLES:\")\n",
        "for idx in high_severity_idx[:2]:\n",
        "    print(f\"\\nReview (rating={df.loc[idx, 'rating']:.0f}):\")\n",
        "    print(f\"{df.loc[idx, 'reviews_combined'][:200]}...\")\n",
        "    print(f\"Semantic severity: {df.loc[idx, 'severity_score']:.3f}\")\n",
        "\n",
        "print(\"\\n2. HIGH SIDE EFFECT EXAMPLES:\")\n",
        "for idx in high_side_effect_idx[:2]:\n",
        "    print(f\"\\nReview (rating={df.loc[idx, 'rating']:.0f}):\")\n",
        "    print(f\"{df.loc[idx, 'reviews_combined'][:200]}...\")\n",
        "    print(f\"Semantic side effect: {df.loc[idx, 'mentions_side_effects']:.3f}\")"
      ],
      "metadata": {
        "id": "w-ihvKdF2-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create patient archetypes for synthetic data generation\n",
        "print(\"\\nCreating patient archetypes...\")\n",
        "\n",
        "cluster_features = df[[\n",
        "    'severity_score',\n",
        "    'mentions_side_effects',\n",
        "    'long_term_use',\n",
        "    'review_length_log',\n",
        "    'sentiment_score',\n",
        "    'rating',\n",
        "]].fillna(0)\n",
        "\n",
        "cluster_scaler = StandardScaler()\n",
        "cluster_features_scaled = cluster_scaler.fit_transform(cluster_features)\n",
        "\n",
        "n_archetypes = 10\n",
        "kmeans = KMeans(n_clusters=n_archetypes, random_state=42, n_init=10)\n",
        "df['patient_archetype'] = kmeans.fit_predict(cluster_features_scaled)\n",
        "\n",
        "print(\"\\nPatient Archetypes:\")\n",
        "for i in range(n_archetypes):\n",
        "    archetype_data = df[df['patient_archetype'] == i]\n",
        "    print(f\"Archetype {i}: n={len(archetype_data)}, \"\n",
        "          f\"avg_rating={archetype_data['rating'].mean():.1f}, \"\n",
        "          f\"severity={archetype_data['severity_score'].mean():.2f}, \"\n",
        "          f\"side_effects={archetype_data['mentions_side_effects'].mean():.2f}\")"
      ],
      "metadata": {
        "id": "TqQaGDFj5y7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nEncoding features...\")\n",
        "\n",
        "condition_encoder = LabelEncoder()\n",
        "drug_encoder = LabelEncoder()\n",
        "\n",
        "df['condition_encoded'] = condition_encoder.fit_transform(df['condition'])\n",
        "df['drug_encoded'] = drug_encoder.fit_transform(df['drugName'])\n",
        "\n",
        "patient_feature_cols = [\n",
        "    'condition_encoded',\n",
        "    'severity_score',\n",
        "    'mentions_side_effects',\n",
        "    'long_term_use',\n",
        "    'review_length_log',\n",
        "    'sentiment_score',\n",
        "    'patient_archetype'\n",
        "]\n",
        "\n",
        "drug_feature_cols = ['drug_encoded']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = [\n",
        "    'severity_score',\n",
        "    'review_length_log',\n",
        "    'sentiment_score'\n",
        "]\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "print(f\"Patient features: {len(patient_feature_cols)}\")"
      ],
      "metadata": {
        "id": "dJOlPDlHHp8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build Rating Prediction Model (Simulator)"
      ],
      "metadata": {
        "id": "atkjOl5PHvVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NN approach requested by Aishwarya\n",
        "class RatingDataset(Dataset):\n",
        "    def __init__(self, patient_features, drug_features, ratings):\n",
        "        self.patient_features = torch.FloatTensor(patient_features)\n",
        "        self.drug_features = torch.FloatTensor(drug_features)\n",
        "        self.ratings = torch.FloatTensor(ratings)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.patient_features[idx], self.drug_features[idx], self.ratings[idx]\n",
        "\n",
        "class RatingPredictor(nn.Module):\n",
        "    def __init__(self, n_patient_features, n_drug_features, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        input_dim = n_patient_features + n_drug_features\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, patient_features, drug_features):\n",
        "        x = torch.cat([patient_features, drug_features], dim=1)\n",
        "        return self.network(x).squeeze() * 0.1 + 5 # centered around 7, given above distribution\n",
        "\n",
        "print(\"Rating prediction model defined\")"
      ],
      "metadata": {
        "id": "_8USg8PNHwFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Preparing DataLoaders and Datasets...\")\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "dataloader_generator = torch.Generator()\n",
        "dataloader_generator.manual_seed(42)\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "def prepare_features(dataframe):\n",
        "    patient_feat = dataframe[patient_feature_cols].values\n",
        "    drug_feat = dataframe[drug_feature_cols].values\n",
        "    ratings = dataframe['rating'].values\n",
        "    return patient_feat, drug_feat, ratings\n",
        "\n",
        "X_train_pat, X_train_drug, y_train = prepare_features(train_df)\n",
        "X_val_pat, X_val_drug, y_val = prepare_features(val_df)\n",
        "X_test_pat, X_test_drug, y_test = prepare_features(test_df)\n",
        "\n",
        "train_dataset = RatingDataset(X_train_pat, X_train_drug, y_train)\n",
        "val_dataset = RatingDataset(X_val_pat, X_val_drug, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, generator=dataloader_generator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, generator=dataloader_generator)\n",
        "\n",
        "print(\"Data preparation complete\")"
      ],
      "metadata": {
        "id": "mYKEmKF_H01F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training rating prediction model...\")\n",
        "\n",
        "n_patient_features = X_train_pat.shape[1]\n",
        "n_drug_features = X_train_drug.shape[1]\n",
        "\n",
        "model = RatingPredictor(n_patient_features, n_drug_features, hidden_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "n_epochs = 50\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for patient_feat, drug_feat, ratings in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(patient_feat, drug_feat)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for patient_feat, drug_feat, ratings in val_loader:\n",
        "            predictions = model(patient_feat, drug_feat)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict().copy()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(\n",
        "        torch.FloatTensor(X_test_pat),\n",
        "        torch.FloatTensor(X_test_drug)\n",
        "    ).numpy()\n",
        "\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "test_rmse = np.sqrt(np.mean((y_test - test_predictions)**2))\n",
        "\n",
        "print(f\"\\n=== Simulator Performance ===\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Baseline: {mean_absolute_error(y_test, np.full_like(y_test, y_train.mean())):.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "plt.plot(val_losses, label='Val Loss', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training/Validation Loss')\n",
        "plt.title('Neural Network Rating Prediction Model Training')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iEIQ74zEH4u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Gradient Boosting (XGBoost)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "X_train_combined = np.concatenate([X_train_pat, X_train_drug], axis=1)\n",
        "X_val_combined = np.concatenate([X_val_pat, X_val_drug], axis=1)\n",
        "X_test_combined = np.concatenate([X_test_pat, X_test_drug], axis=1)\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    X_train_combined, y_train,\n",
        "    eval_set=[(X_val_combined, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "xgb_predictions = xgb_model.predict(X_test_combined)\n",
        "xgb_predictions = np.clip(xgb_predictions, 1, 10)\n",
        "\n",
        "xgb_mae = mean_absolute_error(y_test, xgb_predictions)\n",
        "xgb_rmse = np.sqrt(np.mean((y_test - xgb_predictions)**2))\n",
        "\n",
        "print(f\"XGBoost Test MAE: {xgb_mae:.4f}\")\n",
        "print(f\"XGBoost Test RMSE: {xgb_rmse:.4f}\")\n",
        "\n",
        "feature_importance = xgb_model.feature_importances_\n",
        "print(f\"\\n  Top 3 most important features:\")\n",
        "all_feature_names = patient_feature_cols + drug_feature_cols\n",
        "top_indices = np.argsort(feature_importance)[-3:][::-1]\n",
        "for idx in top_indices:\n",
        "    feat_name = all_feature_names[idx] if idx < len(all_feature_names) else f\"feature_{idx}\"\n",
        "    print(f\"    {feat_name}: {feature_importance[idx]:.4f}\")\n",
        "\n",
        "nn_improvement = test_mae - xgb_mae\n",
        "baseline_mae = mean_absolute_error(y_test, np.full_like(y_test, y_train.mean()))\n",
        "baseline_improvement = baseline_mae - xgb_mae\n",
        "\n",
        "if xgb_mae < test_mae:\n",
        "    print(f\"Improvement over Neural Network: {nn_improvement:.1f}\")\n",
        "    print(f\"Improvement over Baseline: {baseline_improvement:.1f}\")\n",
        "    best_predictions = xgb_predictions\n",
        "    best_mae = xgb_mae\n",
        "    best_model_name = \"XGBoost\"\n",
        "    best_model = xgb_model\n",
        "    use_xgboost = True\n",
        "else:\n",
        "    use_xgboost = False\n",
        "\n",
        "# Store XGBoost feature importance for later reporting\n",
        "xgb_feature_importance = pd.DataFrame({\n",
        "    'Feature': all_feature_names[:len(feature_importance)],\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Store top 3 for Key Findings\n",
        "top_xgb_features = xgb_feature_importance.head(3)"
      ],
      "metadata": {
        "id": "zaM7WtDcLAMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Hybrid Drug-Specific + Patient Model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Training hybrid model...\")\n",
        "\n",
        "# Step 1: Learn drug-specific baseline ratings\n",
        "drug_baselines = df.groupby('drug_encoded')['rating'].agg(['mean', 'std']).to_dict('index')\n",
        "\n",
        "# Step 2: Learn patient deviation model\n",
        "# For each drug, predict how much this patient deviates from drug average\n",
        "class PatientDeviationModel(nn.Module):\n",
        "    def __init__(self, n_patient_features, n_drugs):\n",
        "        super().__init__()\n",
        "        self.drug_baseline = nn.Embedding(n_drugs, 1)\n",
        "        self.deviation_predictor = nn.Sequential(\n",
        "            nn.Linear(n_patient_features, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, patient_features, drug_ids):\n",
        "        baseline = self.drug_baseline(drug_ids).squeeze()\n",
        "        deviation = self.deviation_predictor(patient_features).squeeze() * 3.0\n",
        "\n",
        "        prediction = baseline + deviation\n",
        "        return torch.clamp(prediction, 1.0, 10.0)\n",
        "\n",
        "# Initialize with drug averages\n",
        "hybrid_model = PatientDeviationModel(n_patient_features, len(df['drug_encoded'].unique()))\n",
        "\n",
        "# Initialize drug baselines\n",
        "with torch.no_grad():\n",
        "    for drug_id, stats in drug_baselines.items():\n",
        "        hybrid_model.drug_baseline.weight[drug_id] = stats['mean']\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(hybrid_model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    hybrid_model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for patient_feat, drug_feat, ratings in train_loader:\n",
        "        drug_ids = drug_feat[:, 0].long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = hybrid_model(patient_feat, drug_ids)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    hybrid_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for patient_feat, drug_feat, ratings in val_loader:\n",
        "            drug_ids = drug_feat[:, 0].long()\n",
        "            predictions = hybrid_model(patient_feat, drug_ids)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_hybrid_state = hybrid_model.state_dict().copy()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"   Epoch {epoch+1} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
        "\n",
        "# Load best and evaluate\n",
        "hybrid_model.load_state_dict(best_hybrid_state)\n",
        "hybrid_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_drug_ids = torch.LongTensor(X_test_drug[:, 0].astype(int))\n",
        "    hybrid_predictions = hybrid_model(\n",
        "        torch.FloatTensor(X_test_pat),\n",
        "        test_drug_ids\n",
        "    ).numpy()\n",
        "\n",
        "hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)\n",
        "hybrid_rmse = np.sqrt(np.mean((y_test - hybrid_predictions)**2))\n",
        "\n",
        "print(f\"Hybrid Model Test MAE: {hybrid_mae:.4f}\")\n",
        "print(f\"Hybrid Model Test RMSE: {hybrid_rmse:.4f}\")\n",
        "\n",
        "if hybrid_mae < best_mae:\n",
        "    print(f\"\\nHybrid model is best! Improvement: {((best_mae - hybrid_mae) / best_mae * 100):.1f}%\")\n",
        "    best_predictions = hybrid_predictions\n",
        "    best_mae = hybrid_mae\n",
        "    best_model_name = \"Hybrid Drug+Patient\"\n",
        "    best_model = hybrid_model\n",
        "    use_hybrid = True\n",
        "else:\n",
        "    use_hybrid = False"
      ],
      "metadata": {
        "id": "JlNcjB3rqKq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"APPROACH 3: Ensemble of All Models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_predictions = [test_predictions]\n",
        "prediction_names = [\"Neural Network\"]\n",
        "\n",
        "if use_xgboost:\n",
        "    all_predictions.append(xgb_predictions)\n",
        "    prediction_names.append(\"XGBoost\")\n",
        "\n",
        "if use_hybrid:\n",
        "    all_predictions.append(hybrid_predictions)\n",
        "    prediction_names.append(\"Hybrid\")\n",
        "\n",
        "if len(all_predictions) > 1:\n",
        "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
        "    ensemble_predictions = np.clip(ensemble_predictions, 1, 10)\n",
        "\n",
        "    ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)\n",
        "    ensemble_rmse = np.sqrt(np.mean((y_test - ensemble_predictions)**2))\n",
        "\n",
        "    print(f\"Ensemble of {len(all_predictions)} models: {', '.join(prediction_names)}\")\n",
        "    print(f\"Ensemble Test MAE: {ensemble_mae:.4f}\")\n",
        "    print(f\"Ensemble Test RMSE: {ensemble_rmse:.4f}\")\n",
        "\n",
        "    if ensemble_mae < best_mae:\n",
        "        print(f\"\\nEnsemble is best! Improvement: {((best_mae - ensemble_mae) / best_mae * 100):.1f}%\")\n",
        "        best_predictions = ensemble_predictions\n",
        "        best_mae = ensemble_mae\n",
        "        best_model_name = f\"Ensemble ({len(all_predictions)} models)\"\n",
        "else:\n",
        "    print(\"Only one model available, skipping ensemble\")"
      ],
      "metadata": {
        "id": "zZ31QYrdqSY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generate Synthetic Dataset with Ground Truth"
      ],
      "metadata": {
        "id": "F_BcnKvRIBxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating synthetic dataset...\")\n",
        "\n",
        "print(f\"Using XGBoost model (MAE={xgb_mae:.4f}) for synthetic data generation\\n\")\n",
        "\n",
        "n_synthetic_patients = 2000\n",
        "n_drugs_per_patient = 10\n",
        "\n",
        "synthetic_data = []\n",
        "\n",
        "condition_dist = df['condition_encoded'].value_counts(normalize=True).sort_index()\n",
        "conditions = condition_dist.index.values\n",
        "condition_probs = condition_dist.values\n",
        "\n",
        "severity_dist = df['severity_score'].values\n",
        "side_effects_dist = df['mentions_side_effects'].values\n",
        "long_term_dist = df['long_term_use'].values\n",
        "review_len_dist = df['review_length_log'].values\n",
        "sentiment_dist = df['sentiment_score'].values\n",
        "archetype_dist = df['patient_archetype'].values\n",
        "\n",
        "all_drugs = df['drug_encoded'].unique()\n",
        "\n",
        "print(f\"Generating {n_synthetic_patients} synthetic patients...\")\n",
        "\n",
        "for patient_id in range(n_synthetic_patients):\n",
        "    condition = np.random.choice(conditions, p=condition_probs)\n",
        "    severity = np.random.choice(severity_dist)\n",
        "    side_effects = np.random.choice(side_effects_dist)\n",
        "    long_term = np.random.choice(long_term_dist)\n",
        "    review_len = np.random.choice(review_len_dist)\n",
        "    sentiment = np.random.choice(sentiment_dist)\n",
        "    archetype = np.random.choice(archetype_dist)\n",
        "\n",
        "    patient_features = np.array([condition, severity, side_effects,\n",
        "                                  long_term, review_len, sentiment, archetype])\n",
        "\n",
        "    patient_drugs = np.random.choice(all_drugs, size=min(n_drugs_per_patient, len(all_drugs)),\n",
        "                                      replace=False)\n",
        "\n",
        "    for drug in patient_drugs:\n",
        "        drug_features = np.array([drug])\n",
        "\n",
        "        combined_features = np.concatenate([patient_features, drug_features]).reshape(1, -1)\n",
        "        true_rating = xgb_model.predict(combined_features)[0]\n",
        "        true_rating = np.clip(true_rating, 1, 10)\n",
        "\n",
        "        noisy_rating = true_rating + np.random.normal(0, 0.5) # Ïƒ=0.5 appropriate for MAE~1.25\n",
        "        noisy_rating = np.clip(noisy_rating, 1, 10)\n",
        "\n",
        "        synthetic_data.append({\n",
        "            'patient_id': patient_id,\n",
        "            'condition_encoded': condition,\n",
        "            'severity_score': severity,\n",
        "            'mentions_side_effects': side_effects,\n",
        "            'long_term_use': long_term,\n",
        "            'review_length_log': review_len,\n",
        "            'sentiment_score': sentiment,\n",
        "            'patient_archetype': archetype,\n",
        "            'drug_encoded': drug,\n",
        "            'rating': noisy_rating,\n",
        "            'true_rating': true_rating\n",
        "        })\n",
        "\n",
        "synthetic_df = pd.DataFrame(synthetic_data)\n",
        "print(f\"Generated {len(synthetic_df)} synthetic reviews\")\n",
        "print(f\"Unique patients: {synthetic_df['patient_id'].nunique()}\")\n",
        "print(f\"Unique drugs: {synthetic_df['drug_encoded'].nunique()}\")\n",
        "print(f\"\\nSynthetic ratings - Mean: {synthetic_df['rating'].mean():.2f}, Std: {synthetic_df['rating'].std():.2f}\")\n",
        "print(f\"Compare to real data - Mean: {df['rating'].mean():.2f}, Std: {df['rating'].std():.2f}\")"
      ],
      "metadata": {
        "id": "oyvOCZLRIA_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Constructing pairwise preferences...\")\n",
        "\n",
        "pairwise_data = []\n",
        "\n",
        "for patient_id in synthetic_df['patient_id'].unique():\n",
        "    patient_ratings = synthetic_df[synthetic_df['patient_id'] == patient_id]\n",
        "    patient_features = patient_ratings.iloc[0][[\n",
        "        'condition_encoded', 'severity_score', 'mentions_side_effects',\n",
        "        'long_term_use', 'review_length_log', 'sentiment_score', 'patient_archetype'\n",
        "    ]].values\n",
        "\n",
        "    patient_drugs = patient_ratings['drug_encoded'].values\n",
        "    patient_drug_ratings = patient_ratings['rating'].values\n",
        "    true_ratings = patient_ratings['true_rating'].values\n",
        "\n",
        "    if len(patient_drugs) < 2:\n",
        "        continue\n",
        "\n",
        "    n_pairs = min(5, len(patient_drugs) * (len(patient_drugs) - 1) // 2)\n",
        "    sampled_pairs = np.random.choice(len(list(combinations(range(len(patient_drugs)), 2))),\n",
        "                                      size=n_pairs, replace=False)\n",
        "\n",
        "    for pair_idx in sampled_pairs:\n",
        "        pairs_list = list(combinations(range(len(patient_drugs)), 2))\n",
        "        i, j = pairs_list[pair_idx]\n",
        "\n",
        "        drug_a, drug_b = patient_drugs[i], patient_drugs[j]\n",
        "        rating_a, rating_b = patient_drug_ratings[i], patient_drug_ratings[j]\n",
        "        true_a, true_b = true_ratings[i], true_ratings[j]\n",
        "\n",
        "        utility_diff = rating_a - rating_b\n",
        "        true_utility_diff = true_a - true_b\n",
        "\n",
        "\n",
        "        if abs(utility_diff) > 1.5:\n",
        "            prefers_a = int(utility_diff > 0)\n",
        "        else:\n",
        "            prob_prefer_a = sigmoid(utility_diff * 2)\n",
        "            prefers_a = int(np.random.random() < prob_prefer_a)\n",
        "\n",
        "        pairwise_data.append({\n",
        "            'patient_id': patient_id,\n",
        "            'condition_encoded': patient_features[0],\n",
        "            'severity_score': patient_features[1],\n",
        "            'mentions_side_effects': patient_features[2],\n",
        "            'long_term_use': patient_features[3],\n",
        "            'review_length_log': patient_features[4],\n",
        "            'sentiment_score': patient_features[5],\n",
        "            'patient_archetype': patient_features[6],\n",
        "            'drug_a': drug_a,\n",
        "            'drug_b': drug_b,\n",
        "            'prefers_a': prefers_a,\n",
        "            'true_utility_diff': true_utility_diff\n",
        "        })\n",
        "\n",
        "pairwise_df = pd.DataFrame(pairwise_data)\n",
        "print(f\"Created {len(pairwise_df)} pairwise preferences\")\n",
        "print(f\"Preference distribution: {pairwise_df['prefers_a'].value_counts()}\")\n",
        "\n",
        "# Train/test split for preference learning\n",
        "train_pref, test_pref = train_test_split(pairwise_df, test_size=0.2, random_state=42)\n",
        "print(f\"Train preferences: {len(train_pref)}, Test preferences: {len(test_pref)}\")"
      ],
      "metadata": {
        "id": "ex9c2okFIK6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Implement Bradley-Terry Models"
      ],
      "metadata": {
        "id": "ulKU-LIkIOr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Population Bradley-Terry (no personalization)\n",
        "class PopulationBradleyTerry:\n",
        "    def __init__(self, n_items):\n",
        "        self.n_items = n_items\n",
        "        self.item_utilities = np.zeros(n_items)\n",
        "\n",
        "    def fit(self, item_a, item_b, preferences, lr=0.01, n_epochs=100):\n",
        "        \"\"\"\n",
        "        item_a, item_b: arrays of item indices\n",
        "        preferences: binary array (1 if prefer a, 0 if prefer b)\n",
        "        \"\"\"\n",
        "        for epoch in range(n_epochs):\n",
        "            for i in range(len(item_a)):\n",
        "                a, b, y = item_a[i], item_b[i], preferences[i]\n",
        "\n",
        "                utility_diff = self.item_utilities[a] - self.item_utilities[b]\n",
        "                pred_prob = sigmoid(utility_diff)\n",
        "\n",
        "                error = y - pred_prob\n",
        "                self.item_utilities[a] += lr * error\n",
        "                self.item_utilities[b] -= lr * error\n",
        "\n",
        "    def predict_proba(self, item_a, item_b):\n",
        "        utility_diff = self.item_utilities[item_a] - self.item_utilities[item_b]\n",
        "        return sigmoid(utility_diff)"
      ],
      "metadata": {
        "id": "AQKwbWxaIQ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextualBradleyTerry(nn.Module):\n",
        "    def __init__(self, n_items, n_patient_features):\n",
        "        super().__init__()\n",
        "        self.item_utilities = nn.Embedding(n_items, 1)\n",
        "        self.patient_context = nn.Linear(n_patient_features, 1)\n",
        "        self.interaction = nn.Linear(n_patient_features, n_items)\n",
        "\n",
        "    def forward(self, patient_features, item_a, item_b):\n",
        "        u_a = self.item_utilities(item_a).squeeze()\n",
        "        u_b = self.item_utilities(item_b).squeeze()\n",
        "\n",
        "        interaction_scores = self.interaction(patient_features)\n",
        "        interaction_a = torch.gather(interaction_scores, 1, item_a.unsqueeze(1)).squeeze()\n",
        "        interaction_b = torch.gather(interaction_scores, 1, item_b.unsqueeze(1)).squeeze()\n",
        "\n",
        "        utility_diff = (u_a - u_b) + (interaction_a - interaction_b)\n",
        "\n",
        "        return torch.sigmoid(utility_diff)"
      ],
      "metadata": {
        "id": "3-EMOKx6IVue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train and Evaluate Models"
      ],
      "metadata": {
        "id": "-t1yFlPfIgCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREFERENCE DATA DIAGNOSTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(f\"True utility difference statistics:\")\n",
        "print(f\"Mean: {train_pref['true_utility_diff'].mean():.3f}\")\n",
        "print(f\"Std: {train_pref['true_utility_diff'].std():.3f}\")\n",
        "print(f\"Range: [{train_pref['true_utility_diff'].min():.3f}, {train_pref['true_utility_diff'].max():.3f}]\")\n",
        "\n",
        "agreement = ((train_pref['true_utility_diff'] > 0) == (train_pref['prefers_a'] == 1)).mean()\n",
        "print(f\"\\nTrue utility difference agrees with observed preference: {agreement:.1%}\")\n",
        "\n",
        "print(f\"\\n2. Patient feature variance in preferences:\")\n",
        "for col in patient_feature_cols:\n",
        "    variance = train_pref[col].var()\n",
        "    print(f\"  {col}: {variance:.4f}\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "-xcCkId4oxS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_bt_data(dataframe):\n",
        "    patient_features = dataframe[[\n",
        "        'condition_encoded', 'severity_score', 'mentions_side_effects',\n",
        "        'long_term_use', 'review_length_log', 'sentiment_score', 'patient_archetype'\n",
        "    ]].values\n",
        "\n",
        "    item_a = dataframe['drug_a'].values.astype(int)\n",
        "    item_b = dataframe['drug_b'].values.astype(int)\n",
        "    preferences = dataframe['prefers_a'].values.astype(int)\n",
        "\n",
        "    return patient_features, item_a, item_b, preferences\n",
        "\n",
        "X_train_pref, train_a, train_b, train_y = prepare_bt_data(train_pref)\n",
        "X_test_pref, test_a, test_b, test_y = prepare_bt_data(test_pref)\n",
        "\n",
        "n_drugs = int(max(pairwise_df['drug_a'].max(), pairwise_df['drug_b'].max()) + 1)\n",
        "n_patient_features = X_train_pref.shape[1]\n",
        "\n",
        "print(f\"Number of drugs: {n_drugs}\")\n",
        "print(f\"Patient features: {n_patient_features}\")"
      ],
      "metadata": {
        "id": "j3GKZd5WIhMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Population Bradley-Terry\n",
        "print(\"Training Population Bradley-Terry...\")\n",
        "\n",
        "pop_bt = PopulationBradleyTerry(n_items=n_drugs)\n",
        "pop_bt.fit(train_a, train_b, train_y, lr=0.01, n_epochs=100)\n",
        "\n",
        "pop_train_pred = pop_bt.predict_proba(train_a, train_b)\n",
        "pop_test_pred = pop_bt.predict_proba(test_a, test_b)\n",
        "\n",
        "pop_train_acc = accuracy_score(train_y, (pop_train_pred > 0.5).astype(int))\n",
        "pop_test_acc = accuracy_score(test_y, (pop_test_pred > 0.5).astype(int))\n",
        "\n",
        "print(f\"Population BT - Train Acc: {pop_train_acc:.4f}, Test Acc: {pop_test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "95_uKumwIk5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining Logistic Regression Baseline...\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train_lr = np.concatenate([\n",
        "    X_train_pref,\n",
        "    train_a.reshape(-1, 1),\n",
        "    train_b.reshape(-1, 1)\n",
        "], axis=1)\n",
        "\n",
        "X_test_lr = np.concatenate([\n",
        "    X_test_pref,\n",
        "    test_a.reshape(-1, 1),\n",
        "    test_b.reshape(-1, 1)\n",
        "], axis=1)\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_lr, train_y)\n",
        "\n",
        "lr_train_pred = lr_model.predict_proba(X_train_lr)[:, 1]\n",
        "lr_test_pred = lr_model.predict_proba(X_test_lr)[:, 1]\n",
        "\n",
        "lr_train_acc = accuracy_score(train_y, (lr_train_pred > 0.5).astype(int))\n",
        "lr_test_acc = accuracy_score(test_y, (lr_test_pred > 0.5).astype(int))\n",
        "\n",
        "print(f\"Logistic Regression - Train Acc: {lr_train_acc:.4f}, Test Acc: {lr_test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "qyf-zZrkpmgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Contextual Bradley-Terry\n",
        "print(\"Training Contextual Bradley-Terry...\")\n",
        "\n",
        "contextual_bt = ContextualBradleyTerry(n_items=n_drugs, n_patient_features=n_patient_features)\n",
        "\n",
        "if 'pop_bt' in locals():\n",
        "    with torch.no_grad():\n",
        "        contextual_bt.item_utilities.weight.data = torch.FloatTensor(\n",
        "            pop_bt.item_utilities.reshape(-1, 1)\n",
        "        )\n",
        "\n",
        "optimizer = optim.Adam(contextual_bt.parameters(), lr=0.005, weight_decay=1e-3)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_pref)\n",
        "train_a_tensor = torch.LongTensor(train_a)\n",
        "train_b_tensor = torch.LongTensor(train_b)\n",
        "train_y_tensor = torch.FloatTensor(train_y)\n",
        "\n",
        "X_test_tensor = torch.FloatTensor(X_test_pref)\n",
        "test_a_tensor = torch.LongTensor(test_a)\n",
        "test_b_tensor = torch.LongTensor(test_b)\n",
        "test_y_tensor = torch.FloatTensor(test_y)\n",
        "\n",
        "n_epochs = 150\n",
        "batch_size = 1024\n",
        "n_batches = len(train_y) // batch_size\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "best_test_acc = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    contextual_bt.train()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    indices = np.random.permutation(len(train_y))\n",
        "    for i in range(0, len(train_y), batch_size):\n",
        "        batch_idx = indices[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = contextual_bt(\n",
        "            X_train_tensor[batch_idx],\n",
        "            train_a_tensor[batch_idx],\n",
        "            train_b_tensor[batch_idx]\n",
        "        )\n",
        "        loss = criterion(predictions, train_y_tensor[batch_idx])\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(contextual_bt.parameters(), max_norm=0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        predicted_labels = (predictions > 0.5).float()\n",
        "        correct += (predicted_labels == train_y_tensor[batch_idx]).sum().item()\n",
        "        total += len(batch_idx)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    contextual_bt.eval()\n",
        "    with torch.no_grad():\n",
        "        test_pred = contextual_bt(X_test_tensor, test_a_tensor, test_b_tensor)\n",
        "        test_pred_labels = (test_pred > 0.5).float()\n",
        "        test_acc = (test_pred_labels == test_y_tensor).float().mean().item()\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            best_model_state = contextual_bt.state_dict().copy()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {epoch_loss/n_batches:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "if best_model_state is not None:\n",
        "    contextual_bt.load_state_dict(best_model_state)\n",
        "    print(f\"\\nLoaded best model with test accuracy: {best_test_acc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accs, label='Train Accuracy', alpha=0.7)\n",
        "plt.plot(test_accs, label='Test Accuracy', alpha=0.7)\n",
        "plt.axhline(y=pop_test_acc, color='r', linestyle='--', label=f'Population BT ({pop_test_acc:.3f})', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Contextual BT Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim([0.4, 0.70])\n",
        "plt.show()\n",
        "\n",
        "contextual_bt.eval()\n",
        "with torch.no_grad():\n",
        "    ctx_train_pred = contextual_bt(X_train_tensor, train_a_tensor, train_b_tensor).numpy()\n",
        "    ctx_test_pred = contextual_bt(X_test_tensor, test_a_tensor, test_b_tensor).numpy()\n",
        "\n",
        "ctx_train_acc = accuracy_score(train_y, (ctx_train_pred > 0.5).astype(int))\n",
        "ctx_test_acc = accuracy_score(test_y, (ctx_test_pred > 0.5).astype(int))\n",
        "\n",
        "print(f\"Contextual BT - Train Acc: {ctx_train_acc:.4f}, Test Acc: {ctx_test_acc:.4f}\")\n",
        "print(f\"Population BT - Train Acc: {pop_train_acc:.4f}, Test Acc: {pop_test_acc:.4f}\")\n",
        "\n",
        "if ctx_test_acc > pop_test_acc:\n",
        "    improvement = (ctx_test_acc - pop_test_acc) * 100\n",
        "    print(f\"Contextual BT bests Population BT by {improvement:.4f}%\")\n",
        "else:\n",
        "    decline = (pop_test_acc - ctx_test_acc) * 100\n",
        "    print(f\"Population BT bests Contextual BT by {decline:.4f}%\")"
      ],
      "metadata": {
        "id": "_k9WjvZxIqUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COLD-START ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simulate cold-start: new patients with only demographic info\n",
        "# Use only condition feature (no review-based features)\n",
        "cold_start_features = test_pref[['condition_encoded']].values\n",
        "cold_start_features_padded = np.concatenate([\n",
        "    cold_start_features,\n",
        "    np.zeros((len(cold_start_features), n_patient_features - 1))\n",
        "], axis=1)\n",
        "\n",
        "contextual_bt.eval()\n",
        "with torch.no_grad():\n",
        "    cold_start_pred = contextual_bt(\n",
        "        torch.FloatTensor(cold_start_features_padded),\n",
        "        test_a_tensor,\n",
        "        test_b_tensor\n",
        "    ).numpy()\n",
        "\n",
        "cold_start_acc = accuracy_score(test_y, (cold_start_pred > 0.5).astype(int))\n",
        "\n",
        "print(f\"Cold-start accuracy (condition only): {cold_start_acc:.4f}\")\n",
        "print(f\"Population BT (no personalization): {pop_test_acc:.4f}\")\n",
        "print(f\"Full Contextual BT (all features): {ctx_test_acc:.4f}\")\n",
        "\n",
        "if cold_start_acc > pop_test_acc:\n",
        "    print(\"Even with limited features, personalization helps!\")\n",
        "elif cold_start_acc > 0.50:\n",
        "    print(\"Cold-start performance beats random, suggesting condition-based personalization works\")\n",
        "else:\n",
        "    print(\"Cold-start challenging - need more patient history for personalization\")"
      ],
      "metadata": {
        "id": "xAPKGKthpsyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Results and Visualization"
      ],
      "metadata": {
        "id": "gdK5C7UmIsJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "print(\"\\n=== Utility Recovery Analysis ===\")\n",
        "\n",
        "test_pref_with_pred = test_pref.copy()\n",
        "test_pref_with_pred['pop_pred'] = pop_test_pred\n",
        "test_pref_with_pred['ctx_pred'] = ctx_test_pred\n",
        "\n",
        "eps = 1e-7\n",
        "pop_pred_clipped = np.clip(pop_test_pred, eps, 1-eps)\n",
        "ctx_pred_clipped = np.clip(ctx_test_pred, eps, 1-eps)\n",
        "\n",
        "test_pref_with_pred['pop_utility_diff'] = np.log(pop_pred_clipped / (1 - pop_pred_clipped))\n",
        "test_pref_with_pred['ctx_utility_diff'] = np.log(ctx_pred_clipped / (1 - ctx_pred_clipped))\n",
        "\n",
        "pop_corr, pop_p = spearmanr(test_pref_with_pred['true_utility_diff'],\n",
        "                              test_pref_with_pred['pop_utility_diff'])\n",
        "ctx_corr, ctx_p = spearmanr(test_pref_with_pred['true_utility_diff'],\n",
        "                              test_pref_with_pred['ctx_utility_diff'])\n",
        "\n",
        "print(f\"Population BT - Correlation with true utilities: {pop_corr:.4f} (p={pop_p:.4f})\")\n",
        "print(f\"Contextual BT - Correlation with true utilities: {ctx_corr:.4f} (p={ctx_p:.4f})\")\n",
        "\n",
        "pop_utility_mae = mean_absolute_error(\n",
        "    test_pref_with_pred['true_utility_diff'],\n",
        "    test_pref_with_pred['pop_utility_diff']\n",
        ")\n",
        "ctx_utility_mae = mean_absolute_error(\n",
        "    test_pref_with_pred['true_utility_diff'],\n",
        "    test_pref_with_pred['ctx_utility_diff']\n",
        ")\n",
        "\n",
        "print(f\"\\nUtility Difference MAE:\")\n",
        "print(f\"Population BT: {pop_utility_mae:.4f}\")\n",
        "print(f\"Contextual BT: {ctx_utility_mae:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "xlim = [-5, 5]\n",
        "ylim = [-5, 5]\n",
        "\n",
        "ax[0].scatter(test_pref_with_pred['true_utility_diff'],\n",
        "              test_pref_with_pred['pop_utility_diff'], alpha=0.3, s=10)\n",
        "ax[0].plot(xlim, xlim, 'r--', label='Perfect prediction', linewidth=2)\n",
        "ax[0].set_xlabel('True Utility Difference')\n",
        "ax[0].set_ylabel('Predicted Utility Difference')\n",
        "ax[0].set_title(f'Population BT (Ï={pop_corr:.3f}, MAE={pop_utility_mae:.2f})')\n",
        "ax[0].set_xlim(xlim)\n",
        "ax[0].set_ylim(ylim)\n",
        "ax[0].legend()\n",
        "ax[0].grid(True, alpha=0.3)\n",
        "\n",
        "ax[1].scatter(test_pref_with_pred['true_utility_diff'],\n",
        "              test_pref_with_pred['ctx_utility_diff'], alpha=0.3, s=10)\n",
        "ax[1].plot(xlim, xlim, 'r--', label='Perfect prediction', linewidth=2)\n",
        "ax[1].set_xlabel('True Utility Difference')\n",
        "ax[1].set_ylabel('Predicted Utility Difference')\n",
        "ax[1].set_title(f'Contextual BT (Ï={ctx_corr:.3f}, MAE={ctx_utility_mae:.2f})')\n",
        "ax[1].set_xlim(xlim)\n",
        "ax[1].set_ylim(ylim)\n",
        "ax[1].legend()\n",
        "ax[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kaemmBCLIxb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Feature Importance ===\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    patient_context_weights = contextual_bt.patient_context.weight.squeeze().numpy()\n",
        "\n",
        "    item_utilities = contextual_bt.item_utilities.weight.squeeze().numpy()\n",
        "    interaction_weights = contextual_bt.interaction.weight.numpy()\n",
        "\n",
        "print(f\"\\n=== Model Parameter Statistics ===\")\n",
        "print(f\"Item utilities: mean={item_utilities.mean():.3f}, std={item_utilities.std():.3f}\")\n",
        "print(f\"Patient context: mean={patient_context_weights.mean():.3f}, std={patient_context_weights.std():.3f}\")\n",
        "print(f\"Interactions: mean={interaction_weights.mean():.3f}, std={interaction_weights.std():.3f}\")\n",
        "\n",
        "feature_names = ['Condition', 'Severity', 'Side Effects',\n",
        "                 'Long-term Use', 'Review Length', 'Sentiment', 'Archetype']\n",
        "\n",
        "print(f\"\\nNumber of features: {len(feature_names)}\")\n",
        "print(f\"Number of weights: {len(patient_context_weights)}\")\n",
        "\n",
        "if len(feature_names) != len(patient_context_weights):\n",
        "    print(f\"WARNING: Mismatch! Using generic names.\")\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(len(patient_context_weights))]\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Weight': patient_context_weights,\n",
        "    'Abs_Weight': np.abs(patient_context_weights)\n",
        "}).sort_values('Abs_Weight', ascending=False)\n",
        "\n",
        "print(\"\\n\" + feature_importance[['Feature', 'Weight']].to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if w > 0 else 'red' for w in feature_importance['Weight']]\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Weight'], color=colors)\n",
        "plt.xlabel('Weight (Green=increases utility, Red=decreases)')\n",
        "plt.ylabel('Patient Feature')\n",
        "plt.title('Patient Feature Importance in Contextual BT')\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Interpretation ===\")\n",
        "if feature_importance['Abs_Weight'].max() > 0.01:\n",
        "    for idx, row in feature_importance.head(3).iterrows():\n",
        "        direction = \"increases\" if row['Weight'] > 0 else \"decreases\"\n",
        "        print(f\"- {row['Feature']}: {direction} drug preference (weight={row['Weight']:.3f})\")"
      ],
      "metadata": {
        "id": "46zU5fv1I2xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FEATURE ABLATION STUDY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"Testing impact of removing each feature...\\n\")\n",
        "\n",
        "ablation_results = []\n",
        "\n",
        "for i, feature_name in enumerate(feature_names):\n",
        "    X_test_ablated = X_test_pref.copy()\n",
        "    X_test_ablated[:, i] = 0\n",
        "\n",
        "    contextual_bt.eval()\n",
        "    with torch.no_grad():\n",
        "        ablated_pred = contextual_bt(\n",
        "            torch.FloatTensor(X_test_ablated),\n",
        "            test_a_tensor,\n",
        "            test_b_tensor\n",
        "        ).numpy()\n",
        "\n",
        "    ablated_acc = accuracy_score(test_y, (ablated_pred > 0.5).astype(int))\n",
        "    drop = ctx_test_acc - ablated_acc\n",
        "\n",
        "    ablation_results.append({\n",
        "        'Feature': feature_name,\n",
        "        'Accuracy without': ablated_acc,\n",
        "        'Drop': drop\n",
        "    })\n",
        "\n",
        "    print(f\"{feature_name:20s} - Acc: {ablated_acc:.4f}, Drop: {drop:+.4f}\")\n",
        "\n",
        "ablation_df = pd.DataFrame(ablation_results).sort_values('Drop', ascending=False)\n",
        "\n",
        "print(f\"\\nMost important features (by performance drop):\")\n",
        "for idx, row in ablation_df.head(3).iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Drop']:+.4f}\")"
      ],
      "metadata": {
        "id": "MlW-IdelqxvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Performance by Data Availability ===\")\n",
        "\n",
        "test_patient_counts = test_pref.groupby('patient_id').size()\n",
        "test_pref_with_counts = test_pref.merge(\n",
        "    test_patient_counts.rename('n_comparisons'),\n",
        "    left_on='patient_id',\n",
        "    right_index=True\n",
        ")\n",
        "\n",
        "test_pref_with_counts['pop_correct'] = (pop_test_pred > 0.5).astype(int) == test_y\n",
        "test_pref_with_counts['ctx_correct'] = (ctx_test_pred > 0.5).astype(int) == test_y\n",
        "\n",
        "bins = [0, 3, 5]\n",
        "labels = ['1-2', '3-4']\n",
        "test_pref_with_counts['data_bin'] = pd.cut(test_pref_with_counts['n_comparisons'],\n",
        "                                             bins=bins, labels=labels)\n",
        "\n",
        "performance_by_data = test_pref_with_counts.groupby('data_bin').agg({\n",
        "    'pop_correct': 'mean',\n",
        "    'ctx_correct': 'mean',\n",
        "    'patient_id': 'count'\n",
        "}).rename(columns={'patient_id': 'count'})\n",
        "\n",
        "print(performance_by_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, performance_by_data['pop_correct'], width, label='Population BT')\n",
        "plt.bar(x + width/2, performance_by_data['ctx_correct'], width, label='Contextual BT')\n",
        "\n",
        "plt.xlabel('Number of Comparisons per Patient')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Personalization Effect by Data Availability')\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "plt.ylim([0.5, 1.0])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GpmP2L1tI6Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Key Findings and Conclusions\n"
      ],
      "metadata": {
        "id": "Ci1FMz1gJBom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "1. SIMULATOR PERFORMANCE\n",
        "   - Model: XGBoost (best performer among 3 approaches tested)\n",
        "   - Test MAE: {xgb_mae if 'xgb_mae' in locals() else 'N/A'}\n",
        "   - Test RMSE: {xgb_rmse if 'xgb_rmse' in locals() else 'N/A'}\n",
        "   - Improvement over baseline: {((baseline_mae - xgb_mae) / baseline_mae * 100):.1f}% (neural network: {((baseline_mae - test_mae) / baseline_mae * 100):.1f}%)\n",
        "   - XGBoost substantially outperformed neural network ({xgb_mae:.2f} vs {test_mae:.2f} MAE)\n",
        "\n",
        "2. PERSONALIZATION EFFECT\n",
        "   - Population BT Test Accuracy: {pop_test_acc:.4f}\n",
        "   - Contextual BT Test Accuracy: {ctx_test_acc:.4f}\n",
        "   - Improvement: {improvement:.2f}%\n",
        "   - {'Personalization based on patient covariates improves preference prediction' if improvement > 0 else 'Limited evidence for personalization with current features'}\n",
        "\n",
        "3. UTILITY RECOVERY\n",
        "   - Population BT correlation with true utilities: {pop_corr:.4f}\n",
        "   - Contextual BT correlation with true utilities: {ctx_corr:.4f}\n",
        "   - {'Contextual model better recovers true patient-drug utilities' if ctx_corr > pop_corr else 'Similar utility recovery between models'}\n",
        "\n",
        "4. IMPORTANT PATIENT FEATURES\n",
        "   Top features predicting treatment preferences:\n",
        "\"\"\")\n",
        "\n",
        "for idx, row in feature_importance.head(3).iterrows():\n",
        "    print(f\"   - {row['Feature']}: {row['Weight']:.4f}\")\n",
        "\n",
        "print(f\"\"\"\n",
        "5. DATA AVAILABILITY\n",
        "   - Personalization most effective with 5+ comparisons per patient\n",
        "   - Limited benefit with <3 comparisons (insufficient data to estimate patient effects)\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "cVnI0TwHI-XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMfgILwP6-vz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
